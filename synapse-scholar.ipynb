{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tejzdev/synapse-scholar?scriptVersionId=280073899\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Setup and API Configuration\n\nThis is the initialization cell. It handles all necessary groundwork before any LLM calls can be made. First, it imports essential Python libraries like os, asyncio, time, and datetime. Crucially, it imports google.generativeai to interface with the Gemini API. Since this notebook is running in a secure environment, it uses **kaggle_secrets** to securely retrieve the **GOOGLE_API_KEY**. The script then sets this key as an environment variable and configures the genai client, making the API ready to use. Finally, **nest_asyncio.apply()** is called to patch the environment, which is often required to run asynchronous code (like API calls) smoothly within a Jupyter notebook.","metadata":{}},{"cell_type":"code","source":"# This code is written entirely by Tejas K\n\nimport os\nimport google.generativeai as genai\nimport nest_asyncio\nimport asyncio\nimport time\nfrom datetime import datetime\nimport json\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nGOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\nnest_asyncio.apply()\n\n# Load API key from Kaggle Secrets\ngenai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n\nprint(\"Google Gemini configured successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:29:21.16165Z","iopub.execute_input":"2025-11-19T20:29:21.162043Z","iopub.status.idle":"2025-11-19T20:29:27.427749Z","shell.execute_reply.started":"2025-11-19T20:29:21.162015Z","shell.execute_reply":"2025-11-19T20:29:27.426645Z"}},"outputs":[{"name":"stdout","text":"Google Gemini configured successfully!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## LLM Wrapper Function\n\nThis cell defines the primary, reusable interface for interacting with the Gemini model. It sets a constant **MODEL_NAME** to **\"gemini-2.0-flash\"**. This model is chosen for its balance of high quality and low latency, making it ideal for the rapid interactions required in a multi-agent system. The asynchronous function **llm(prompt)** acts as a simple wrapper: it initializes the specified Gemini model, sends the user's prompt using **generate_content()**, and returns the plain generated text, streamlining the process for all subsequent agent calls.","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"gemini-2.0-flash\"\n\nasync def llm(prompt):\n    \"\"\"Simple wrapper for Gemini calls.\"\"\"\n    response = genai.GenerativeModel(MODEL_NAME).generate_content(prompt)\n    return response.text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:29:27.429492Z","iopub.execute_input":"2025-11-19T20:29:27.429957Z","iopub.status.idle":"2025-11-19T20:29:27.435307Z","shell.execute_reply.started":"2025-11-19T20:29:27.42993Z","shell.execute_reply":"2025-11-19T20:29:27.434214Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Context\nThis cell establishes variables and helper functions crucial for maintaining the conversation's state and history. The **memory_bank** is a simple list used for general logging or storing high-level interaction history. The session_state dictionary is the central hub for the user's session, holding parameters like the difficulty level for quizzes and a list of messages for context. The **compact_context** function is a crucial piece of Context Engineering. It ensures that only the last eight messages (max_len=8) are kept in the context history. This mechanism prevents the LLM from processing an excessively long chat history, which saves on token usage and dramatically reduces the response latency.","metadata":{}},{"cell_type":"code","source":"memory_bank = []\nsession_state = {\n    \"messages\": [],\n    \"difficulty\": \"medium\",\n    \"user_name\": None\n}\n\ndef add_to_memory(entry):\n    memory_bank.append({\n        \"timestamp\": datetime.now().isoformat(),\n        \"data\": entry\n    })\n\ndef compact_context(messages, max_len=8):\n    \"\"\"Context engineering: keep last N messages.\"\"\"\n    return messages[-max_len:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:29:27.436471Z","iopub.execute_input":"2025-11-19T20:29:27.436969Z","iopub.status.idle":"2025-11-19T20:29:27.462364Z","shell.execute_reply.started":"2025-11-19T20:29:27.436937Z","shell.execute_reply":"2025-11-19T20:29:27.461166Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Logs and Metrics\nTo monitor and debug the complex agent flow, this cell sets up two tracking systems. The logs list stores detailed, time-stamped records of every major event, especially when an agent is triggered. The metrics dictionary tracks key statistics like **total_agent_calls** and **tool_calls**, providing insight into the system's performance and component usage. The log function provides a convenient way to add entries to the logs list and print a real-time console message.","metadata":{}},{"cell_type":"code","source":"logs = []\nmetrics = {\n    \"total_agent_calls\": 0,\n    \"tool_calls\": 0,\n    \"parallel_runs\": 0,\n    \"sequential_runs\": 0\n}\n\ndef log(event, extra=None):\n    logs.append({\n        \"event\": event,\n        \"extra\": extra,\n        \"time\": datetime.now().isoformat()\n    })\n    print(f\"[LOG] {event}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:29:27.464029Z","iopub.execute_input":"2025-11-19T20:29:27.464299Z","iopub.status.idle":"2025-11-19T20:29:27.482813Z","shell.execute_reply.started":"2025-11-19T20:29:27.464275Z","shell.execute_reply":"2025-11-19T20:29:27.481742Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Some external tools\n**Google Search(query):** This function uses an external, public search API (aiohttp is used for the web request) to fetch up-to-date information from the internet. It increments the tool_calls metric.\n\n**calculator(expression):** This function uses Python's built-in eval() to execute mathematical expressions directly, providing highly reliable results for math queries. It also increments the tool_calls metric and includes basic error handling for invalid input.","metadata":{}},{"cell_type":"code","source":"import aiohttp\n\nasync def google_search(query):\n    \"\"\"Free search using a public search API.\"\"\"\n    metrics[\"tool_calls\"] += 1\n    url = f\"https://ddg-api.herokuapp.com/search?query={query}\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as resp:\n            data = await resp.json()\n            return data.get(\"results\", [])\n\nasync def calculator(expression):\n    metrics[\"tool_calls\"] += 1\n    try:\n        return eval(expression)\n    except:\n        return \"Invalid expression\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:29:27.485371Z","iopub.execute_input":"2025-11-19T20:29:27.48571Z","iopub.status.idle":"2025-11-19T20:29:27.505428Z","shell.execute_reply.started":"2025-11-19T20:29:27.485685Z","shell.execute_reply":"2025-11-19T20:29:27.504179Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Defining Agents\n**agent_query_understanding(user_input):** This is the Intent Router Agent. Its sole purpose is to classify the user's input into one of six categories (explain, summarize, quiz, research, chat, math). The outcome of this agent call dictates which path the program follows.\n\n**agent_explainer(question):** Provides simple, direct explanations of concepts.\n\n**agent_summary(text):** Produces concise summaries suitable for revision.\n\n**agent_quiz(topic, difficulty):** Generates a 5-question quiz based on the user's topic and the current session difficulty setting.\n\n**agent_research(query):** This is the Grounding Agent. It first calls the external Google Search tool (Cell 5) to gather search results, then sends those results to the LLM to synthesize a coherent, grounded answer.\n\n**agent_evaluator(answer):** A defined agent for the purpose of assessing student answers, although it is not currently connected to the main router flow.","metadata":{}},{"cell_type":"code","source":"async def agent_query_understanding(user_input):\n    metrics[\"total_agent_calls\"] += 1\n    log(\"Query Understanding Agent triggered\")\n\n    prompt = f\"\"\"\n    You are the Query Understanding Agent for Synapse Scholar.\n    Classify the user message into one category:\n    (explain, summarize, quiz, research, chat, math)\n\n    Message: {user_input}\n\n    Return ONLY the category.\n    \"\"\"\n    return await llm(prompt)\n\nasync def agent_explainer(question):\n    metrics[\"total_agent_calls\"] += 1\n    log(\"Explainer Agent triggered\")\n    return await llm(f\"Explain this concept simply for a student:\\n{question}\")\n\nasync def agent_summary(text):\n    metrics[\"total_agent_calls\"] += 1\n    log(\"Summarizer Agent triggered\")\n    return await llm(f\"Summarize this text for revision:\\n{text}\")\n\nasync def agent_quiz(topic, difficulty):\n    metrics[\"total_agent_calls\"] += 1\n    log(\"Quiz Generator Agent triggered\")\n    return await llm(f\"Create a {difficulty} level quiz (5 questions) on:\\n{topic}\")\n\nasync def agent_research(query):\n    metrics[\"total_agent_calls\"] += 1\n    log(\"Research Agent triggered\")\n\n    results = await google_search(query)\n    text = \"\\n\".join([r.get(\"title\",\"\") + \": \" + r.get(\"body\",\"\") for r in results])\n    return await llm(f\"Based on this data, provide an answer:\\n{text}\")\n\nasync def agent_evaluator(answer):\n    metrics[\"total_agent_calls\"] += 1\n    log(\"Evaluator Agent triggered\")\n    return await llm(f\"Evaluate the student answer:\\n{answer}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:29:27.50665Z","iopub.execute_input":"2025-11-19T20:29:27.507019Z","iopub.status.idle":"2025-11-19T20:29:27.520761Z","shell.execute_reply.started":"2025-11-19T20:29:27.506986Z","shell.execute_reply":"2025-11-19T20:29:27.519704Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## The Router\n\nThe **router(user_input)** function is the system's Control Plane.\n\nIt updates the **session_state** with the new input and compacts the context history.\n\nIt calls the **agent_query_understanding** to determine the user's intent.\n\nIt then uses a simple if/elif block to route the request:\n\nIf the intent is explain, it calls **agent_explainer**.\n\nIf the intent is research, it calls **agent_research**.\n\nIf the intent is math, it calls the non-LLM calculator tool for direct computation.\n\nIf the intent doesn't match a specialized task (like chat), it falls back to the base llm wrapper for a general conversational response.","metadata":{}},{"cell_type":"code","source":"async def router(user_input):\n    session_state[\"messages\"].append(user_input)\n    session_state[\"messages\"] = compact_context(session_state[\"messages\"])\n\n    intent = (await agent_query_understanding(user_input)).lower()\n\n    if \"explain\" in intent:\n        return await agent_explainer(user_input)\n    elif \"summarize\" in intent:\n        return await agent_summary(user_input)\n    elif \"quiz\" in intent:\n        return await agent_quiz(user_input, session_state[\"difficulty\"])\n    elif \"research\" in intent:\n        return await agent_research(user_input)\n    elif \"math\" in intent:\n        result = await calculator(user_input)\n        return f\"Math Result: {result}\"\n    else:\n        return await llm(user_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:29:27.521918Z","iopub.execute_input":"2025-11-19T20:29:27.522177Z","iopub.status.idle":"2025-11-19T20:29:27.548611Z","shell.execute_reply.started":"2025-11-19T20:29:27.522158Z","shell.execute_reply":"2025-11-19T20:29:27.547596Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## MAIN EXECUTION LOOP\nThis cell contains the *synapse_scholar()* function, which is the main conversational driver. It runs an infinite while True loop that continuously prompts the user for input. When the user types **\"exit\"**, the loop breaks. For every query, it records the start time, calls the router to get the response, records the end time, and prints the response along with the total latency (response time). The final line, **await synapse_scholar()**, executes the application and starts the chat session.","metadata":{}},{"cell_type":"code","source":"async def synapse_scholar():\n    print(\"üß† Synapse Scholar Online. Type 'exit' to quit.\\n\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            break\n\n        start = time.time()\n        response = await router(user_input)\n        end = time.time()\n\n        print(f\"\\nSynapse Scholar ({end-start:.2f}s):\\n{response}\\n\")\n\nawait synapse_scholar()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:29:27.549755Z","iopub.execute_input":"2025-11-19T20:29:27.550161Z","iopub.status.idle":"2025-11-19T20:29:43.135226Z","shell.execute_reply.started":"2025-11-19T20:29:27.550099Z","shell.execute_reply":"2025-11-19T20:29:43.134154Z"}},"outputs":[{"name":"stdout","text":"üß† Synapse Scholar Online. Type 'exit' to quit.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  who was steve jobs?\n"},{"name":"stdout","text":"[LOG] Query Understanding Agent triggered\n[LOG] Explainer Agent triggered\n\nSynapse Scholar (2.93s):\nOkay, imagine you have an iPhone, iPad, or use a Mac computer. These cool gadgets are famous for being easy to use and looking great.\n\nSteve Jobs was the guy who was the *big boss* behind these things. He was the co-founder and CEO of a company called Apple.\n\nThink of it like this:\n\n*   **He had a vision:** He imagined technology that was simple, beautiful, and changed the way people lived. He didn't just want to make computers; he wanted to make *amazing* computers that everyone wanted.\n\n*   **He was a perfectionist:** He cared a lot about the details, even the tiny ones, to make sure everything was perfect. He pushed his team really hard to make the best products possible.\n\n*   **He was good at presenting:** He knew how to show off his products and get people excited about them. He was famous for his presentations where he would unveil new Apple devices.\n\nSo, in short, Steve Jobs was a very influential person who helped create Apple and made some of the most popular and recognizable tech products in the world. He's known for his vision, his passion for design, and his ability to make technology that everyone could understand and enjoy.\n\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"}],"execution_count":8},{"cell_type":"markdown","source":"## Reporting and Metrics Display\nThis is a utility cell designed to be run after the chat session has ended. It prints a structured, easy-to-read summary of the session using the data collected in Cells 3 and 4:\n\n1. It displays the metrics (tool calls, agent calls).\n\n2. It displays the contents of the memory_bank.\n\nIt displays the last 10 detailed logs from the session, which is invaluable for post-session analysis and debugging the agent workflow.","metadata":{}},{"cell_type":"code","source":"print(\"====LOGS====\")\n\nprint(\"üìä METRICS\")\nprint(json.dumps(metrics, indent=2))\n\nprint(\"\\nüìù MEMORY BANK\")\nprint(json.dumps(memory_bank, indent=2))\n\nprint(\"\\nüîç LOGS (Last 10)\")\nprint(json.dumps(logs[-10:], indent=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T20:29:43.136197Z","iopub.execute_input":"2025-11-19T20:29:43.136467Z","iopub.status.idle":"2025-11-19T20:29:43.143938Z","shell.execute_reply.started":"2025-11-19T20:29:43.136444Z","shell.execute_reply":"2025-11-19T20:29:43.142488Z"}},"outputs":[{"name":"stdout","text":"====LOGS====\nüìä METRICS\n{\n  \"total_agent_calls\": 2,\n  \"tool_calls\": 0,\n  \"parallel_runs\": 0,\n  \"sequential_runs\": 0\n}\n\nüìù MEMORY BANK\n[]\n\nüîç LOGS (Last 10)\n[\n  {\n    \"event\": \"Query Understanding Agent triggered\",\n    \"extra\": null,\n    \"time\": \"2025-11-19T20:29:35.338882\"\n  },\n  {\n    \"event\": \"Explainer Agent triggered\",\n    \"extra\": null,\n    \"time\": \"2025-11-19T20:29:36.135667\"\n  }\n]\n","output_type":"stream"}],"execution_count":9}]}